2018-07-05 15:20:54.964973

==== PARAMETERS:
experiment_name: e0007
seed: 42
with_cuda: True
path_save: experiments/
TRAIN: 
  resume: 
  epochs: 30
  lr: 0.001
  momentum: 0.9
MODEL: 
  name: lenet_in3x32x32_out10
DATASET: 
  name: fashion
  path: data/deepfashion
  batch_size: 32
  batch_size_val: 64
  download: False
LOG: 
  iter_interval: 10
  path: experiments/
  visdom: True
device: cpu


==== NET MODEL:
LeNet3x32x32(
  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=400, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
==== OPTIMIZER:
SGD (
Parameter Group 0
    dampening: 0
    lr: 0.001
    momentum: 0.9
    nesterov: False
    weight_decay: 0
)

Epoch[0] Iteration[0/219] Loss: 2.28 Time: 00:00:00:03
Epoch[0] Iteration[10/219] Loss: 2.29 Time: 00:00:00:04
Epoch[0] Iteration[20/219] Loss: 2.27 Time: 00:00:00:04
Epoch[0] Iteration[30/219] Loss: 2.25 Time: 00:00:00:05
Epoch[0] Iteration[40/219] Loss: 2.26 Time: 00:00:00:05
Epoch[0] Iteration[50/219] Loss: 2.20 Time: 00:00:00:06
Epoch[0] Iteration[60/219] Loss: 2.21 Time: 00:00:00:06
Epoch[0] Iteration[70/219] Loss: 2.17 Time: 00:00:00:07
Epoch[0] Iteration[80/219] Loss: 2.17 Time: 00:00:00:07
Epoch[0] Iteration[90/219] Loss: 2.19 Time: 00:00:00:08
Epoch[0] Iteration[100/219] Loss: 2.14 Time: 00:00:00:08
Epoch[0] Iteration[110/219] Loss: 2.09 Time: 00:00:00:09
Epoch[0] Iteration[120/219] Loss: 2.00 Time: 00:00:00:09
Epoch[0] Iteration[130/219] Loss: 2.00 Time: 00:00:00:10
Epoch[0] Iteration[140/219] Loss: 2.00 Time: 00:00:00:11
Epoch[0] Iteration[150/219] Loss: 1.73 Time: 00:00:00:11
Epoch[0] Iteration[160/219] Loss: 1.86 Time: 00:00:00:12
Epoch[0] Iteration[170/219] Loss: 1.96 Time: 00:00:00:12
Epoch[0] Iteration[180/219] Loss: 1.93 Time: 00:00:00:12
Epoch[0] Iteration[190/219] Loss: 1.59 Time: 00:00:00:13
Epoch[0] Iteration[200/219] Loss: 1.76 Time: 00:00:00:13
Epoch[0] Iteration[210/219] Loss: 1.68 Time: 00:00:00:14
Epoch: 1  Train Avg accuracy: 31.41 Train  Avg loss: 1.67 Validation Avg accuracy: 27.97 Validation Avg loss: 1.70 Time: 00:00:00:32 BEST MODEL SAVED
Epoch[1] Iteration[0/219] Loss: 1.78 Time: 00:00:00:35
Epoch[1] Iteration[10/219] Loss: 1.59 Time: 00:00:00:36
Epoch[1] Iteration[20/219] Loss: 1.57 Time: 00:00:00:36
Epoch[1] Iteration[30/219] Loss: 1.64 Time: 00:00:00:37
Epoch[1] Iteration[40/219] Loss: 1.52 Time: 00:00:00:37
Epoch[1] Iteration[50/219] Loss: 1.56 Time: 00:00:00:38
Epoch[1] Iteration[60/219] Loss: 1.82 Time: 00:00:00:38
Epoch[1] Iteration[70/219] Loss: 1.64 Time: 00:00:00:39
Epoch[1] Iteration[80/219] Loss: 1.63 Time: 00:00:00:39
Epoch[1] Iteration[90/219] Loss: 1.49 Time: 00:00:00:40
Epoch[1] Iteration[100/219] Loss: 1.30 Time: 00:00:00:40
Epoch[1] Iteration[110/219] Loss: 1.71 Time: 00:00:00:41
Epoch[1] Iteration[120/219] Loss: 1.55 Time: 00:00:00:41
Epoch[1] Iteration[130/219] Loss: 1.68 Time: 00:00:00:42
Epoch[1] Iteration[140/219] Loss: 1.65 Time: 00:00:00:42
Epoch[1] Iteration[150/219] Loss: 1.32 Time: 00:00:00:42
Epoch[1] Iteration[160/219] Loss: 1.40 Time: 00:00:00:43
Epoch[1] Iteration[170/219] Loss: 1.44 Time: 00:00:00:43
Epoch[1] Iteration[180/219] Loss: 1.52 Time: 00:00:00:44
Epoch[1] Iteration[190/219] Loss: 1.51 Time: 00:00:00:44
Epoch[1] Iteration[200/219] Loss: 1.42 Time: 00:00:00:45
Epoch[1] Iteration[210/219] Loss: 1.57 Time: 00:00:00:45
Epoch: 2  Train Avg accuracy: 41.18 Train  Avg loss: 1.43 Validation Avg accuracy: 37.13 Validation Avg loss: 1.50 Time: 00:00:01:02 BEST MODEL SAVED
Epoch[2] Iteration[0/219] Loss: 1.42 Time: 00:00:01:05
Epoch[2] Iteration[10/219] Loss: 1.62 Time: 00:00:01:06
Epoch[2] Iteration[20/219] Loss: 1.55 Time: 00:00:01:06
Epoch[2] Iteration[30/219] Loss: 1.59 Time: 00:00:01:07
Epoch[2] Iteration[40/219] Loss: 1.54 Time: 00:00:01:07
Epoch[2] Iteration[50/219] Loss: 0.99 Time: 00:00:01:08
Epoch[2] Iteration[60/219] Loss: 1.27 Time: 00:00:01:08
Epoch[2] Iteration[70/219] Loss: 1.33 Time: 00:00:01:09
Epoch[2] Iteration[80/219] Loss: 1.35 Time: 00:00:01:09
Epoch[2] Iteration[90/219] Loss: 1.35 Time: 00:00:01:10
Epoch[2] Iteration[100/219] Loss: 1.24 Time: 00:00:01:10
Epoch[2] Iteration[110/219] Loss: 1.15 Time: 00:00:01:11
Epoch[2] Iteration[120/219] Loss: 1.20 Time: 00:00:01:11
Epoch[2] Iteration[130/219] Loss: 1.34 Time: 00:00:01:12
Epoch[2] Iteration[140/219] Loss: 1.21 Time: 00:00:01:12
Epoch[2] Iteration[150/219] Loss: 1.22 Time: 00:00:01:13
Epoch[2] Iteration[160/219] Loss: 1.62 Time: 00:00:01:13
Epoch[2] Iteration[170/219] Loss: 1.44 Time: 00:00:01:14
Epoch[2] Iteration[180/219] Loss: 1.11 Time: 00:00:01:14
Epoch[2] Iteration[190/219] Loss: 1.27 Time: 00:00:01:15
Epoch[2] Iteration[200/219] Loss: 1.39 Time: 00:00:01:15
Epoch[2] Iteration[210/219] Loss: 1.12 Time: 00:00:01:15
Epoch: 3  Train Avg accuracy: 45.91 Train  Avg loss: 1.33 Validation Avg accuracy: 43.07 Validation Avg loss: 1.41 Time: 00:00:01:36 BEST MODEL SAVED
Epoch[3] Iteration[0/219] Loss: 1.29 Time: 00:00:01:40
Epoch[3] Iteration[10/219] Loss: 1.30 Time: 00:00:01:41
Epoch[3] Iteration[20/219] Loss: 1.50 Time: 00:00:01:42
Epoch[3] Iteration[30/219] Loss: 1.55 Time: 00:00:01:42
Epoch[3] Iteration[40/219] Loss: 1.42 Time: 00:00:01:43
Epoch[3] Iteration[50/219] Loss: 1.14 Time: 00:00:01:43
Epoch[3] Iteration[60/219] Loss: 1.05 Time: 00:00:01:44
Epoch[3] Iteration[70/219] Loss: 1.37 Time: 00:00:01:45
Epoch[3] Iteration[80/219] Loss: 1.37 Time: 00:00:01:45
Epoch[3] Iteration[90/219] Loss: 1.52 Time: 00:00:01:46
Epoch[3] Iteration[100/219] Loss: 1.28 Time: 00:00:01:47
Epoch[3] Iteration[110/219] Loss: 1.43 Time: 00:00:01:47
Epoch[3] Iteration[120/219] Loss: 1.24 Time: 00:00:01:48
Epoch[3] Iteration[130/219] Loss: 1.36 Time: 00:00:01:49
Epoch[3] Iteration[140/219] Loss: 1.45 Time: 00:00:01:50
Epoch[3] Iteration[150/219] Loss: 1.49 Time: 00:00:01:50
Epoch[3] Iteration[160/219] Loss: 1.26 Time: 00:00:01:51
Epoch[3] Iteration[170/219] Loss: 1.54 Time: 00:00:01:52
Epoch[3] Iteration[180/219] Loss: 1.22 Time: 00:00:01:52
Epoch[3] Iteration[190/219] Loss: 1.42 Time: 00:00:01:53
Epoch[3] Iteration[200/219] Loss: 1.38 Time: 00:00:01:54
Epoch[3] Iteration[210/219] Loss: 1.37 Time: 00:00:01:54
Epoch: 4  Train Avg accuracy: 47.80 Train  Avg loss: 1.29 Validation Avg accuracy: 44.30 Validation Avg loss: 1.35 Time: 00:00:02:16 BEST MODEL SAVED
Epoch[4] Iteration[0/219] Loss: 1.52 Time: 00:00:02:20
Epoch[4] Iteration[10/219] Loss: 1.15 Time: 00:00:02:21
Epoch[4] Iteration[20/219] Loss: 1.31 Time: 00:00:02:21
Epoch[4] Iteration[30/219] Loss: 1.22 Time: 00:00:02:22
Epoch[4] Iteration[40/219] Loss: 1.33 Time: 00:00:02:22
Epoch[4] Iteration[50/219] Loss: 1.45 Time: 00:00:02:23
Epoch[4] Iteration[60/219] Loss: 1.26 Time: 00:00:02:24
Epoch[4] Iteration[70/219] Loss: 1.31 Time: 00:00:02:24
Epoch[4] Iteration[80/219] Loss: 1.23 Time: 00:00:02:25
Epoch[4] Iteration[90/219] Loss: 1.21 Time: 00:00:02:25
Epoch[4] Iteration[100/219] Loss: 1.51 Time: 00:00:02:26
Epoch[4] Iteration[110/219] Loss: 1.18 Time: 00:00:02:27
Epoch[4] Iteration[120/219] Loss: 1.14 Time: 00:00:02:27
Epoch[4] Iteration[130/219] Loss: 1.42 Time: 00:00:02:28
Epoch[4] Iteration[140/219] Loss: 1.75 Time: 00:00:02:28
Epoch[4] Iteration[150/219] Loss: 1.31 Time: 00:00:02:29
Epoch[4] Iteration[160/219] Loss: 1.22 Time: 00:00:02:30
Epoch[4] Iteration[170/219] Loss: 1.05 Time: 00:00:02:30
Epoch[4] Iteration[180/219] Loss: 0.93 Time: 00:00:02:31
Epoch[4] Iteration[190/219] Loss: 1.13 Time: 00:00:02:32
Epoch[4] Iteration[200/219] Loss: 1.18 Time: 00:00:02:32
Epoch[4] Iteration[210/219] Loss: 1.18 Time: 00:00:02:33
Epoch: 5  Train Avg accuracy: 47.49 Train  Avg loss: 1.25 Validation Avg accuracy: 46.18 Validation Avg loss: 1.32 Time: 00:00:02:51 BEST MODEL SAVED
Epoch[5] Iteration[0/219] Loss: 1.18 Time: 00:00:02:54
Epoch[5] Iteration[10/219] Loss: 1.35 Time: 00:00:02:55
Epoch[5] Iteration[20/219] Loss: 1.28 Time: 00:00:02:55
Epoch[5] Iteration[30/219] Loss: 1.15 Time: 00:00:02:56
Epoch[5] Iteration[40/219] Loss: 1.24 Time: 00:00:02:56
Epoch[5] Iteration[50/219] Loss: 1.28 Time: 00:00:02:57
Epoch[5] Iteration[60/219] Loss: 1.31 Time: 00:00:02:57
Epoch[5] Iteration[70/219] Loss: 1.41 Time: 00:00:02:58
Epoch[5] Iteration[80/219] Loss: 1.24 Time: 00:00:02:58
Epoch[5] Iteration[90/219] Loss: 1.13 Time: 00:00:02:59
Epoch[5] Iteration[100/219] Loss: 1.46 Time: 00:00:02:59
Epoch[5] Iteration[110/219] Loss: 1.13 Time: 00:00:03:00
Epoch[5] Iteration[120/219] Loss: 1.33 Time: 00:00:03:00
Epoch[5] Iteration[130/219] Loss: 1.26 Time: 00:00:03:01
Epoch[5] Iteration[140/219] Loss: 1.25 Time: 00:00:03:01
Epoch[5] Iteration[150/219] Loss: 1.04 Time: 00:00:03:02
Epoch[5] Iteration[160/219] Loss: 1.17 Time: 00:00:03:02
Epoch[5] Iteration[170/219] Loss: 0.88 Time: 00:00:03:03
Epoch[5] Iteration[180/219] Loss: 1.34 Time: 00:00:03:03
Epoch[5] Iteration[190/219] Loss: 1.33 Time: 00:00:03:04
Epoch[5] Iteration[200/219] Loss: 1.40 Time: 00:00:03:04
Epoch[5] Iteration[210/219] Loss: 1.69 Time: 00:00:03:05
Epoch: 6  Train Avg accuracy: 50.20 Train  Avg loss: 1.22 Validation Avg accuracy: 45.36 Validation Avg loss: 1.31 Time: 00:00:03:21
Epoch[6] Iteration[0/219] Loss: 1.15 Time: 00:00:03:24
Epoch[6] Iteration[10/219] Loss: 1.26 Time: 00:00:03:24
Epoch[6] Iteration[20/219] Loss: 1.47 Time: 00:00:03:25
Epoch[6] Iteration[30/219] Loss: 1.21 Time: 00:00:03:25
Epoch[6] Iteration[40/219] Loss: 1.53 Time: 00:00:03:26
Epoch[6] Iteration[50/219] Loss: 1.08 Time: 00:00:03:26
Epoch[6] Iteration[60/219] Loss: 1.47 Time: 00:00:03:27
Epoch[6] Iteration[70/219] Loss: 1.15 Time: 00:00:03:27
Epoch[6] Iteration[80/219] Loss: 1.05 Time: 00:00:03:28
Epoch[6] Iteration[90/219] Loss: 1.02 Time: 00:00:03:29
Epoch[6] Iteration[100/219] Loss: 0.99 Time: 00:00:03:29
Epoch[6] Iteration[110/219] Loss: 1.15 Time: 00:00:03:30
Epoch[6] Iteration[120/219] Loss: 1.30 Time: 00:00:03:30
Epoch[6] Iteration[130/219] Loss: 1.25 Time: 00:00:03:31
Epoch[6] Iteration[140/219] Loss: 1.32 Time: 00:00:03:31
Epoch[6] Iteration[150/219] Loss: 1.38 Time: 00:00:03:32
Epoch[6] Iteration[160/219] Loss: 1.31 Time: 00:00:03:32
Epoch[6] Iteration[170/219] Loss: 1.58 Time: 00:00:03:33
Epoch[6] Iteration[180/219] Loss: 1.33 Time: 00:00:03:33
Epoch[6] Iteration[190/219] Loss: 1.29 Time: 00:00:03:34
Epoch[6] Iteration[200/219] Loss: 1.19 Time: 00:00:03:34
Epoch[6] Iteration[210/219] Loss: 1.11 Time: 00:00:03:35
