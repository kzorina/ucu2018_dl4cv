2018-07-29 09:59:14.918941

==== PARAMETERS:
seed: 42
DATASET: 
  batch_size_val: 64
  name: fashion
  batch_size: 32
  download: False
  path: data/deepfashion
experiment_name: e0049
path_save: experiments/
MODEL: 
  name: vgg19
LOG: 
  iter_interval: 10
  path: experiments/
  visdom: False
TRAIN: 
  resume: 
  epochs: 30
  lr: 0.001
  momentum: 0.9
device: cuda
with_cuda: True


==== NET MODEL:
VGG(
  (features): DataParallel(
    (module): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ReLU(inplace)
      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (3): ReLU(inplace)
      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (6): ReLU(inplace)
      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (8): ReLU(inplace)
      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (11): ReLU(inplace)
      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (13): ReLU(inplace)
      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (15): ReLU(inplace)
      (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (17): ReLU(inplace)
      (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (20): ReLU(inplace)
      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (22): ReLU(inplace)
      (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (24): ReLU(inplace)
      (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (26): ReLU(inplace)
      (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (29): ReLU(inplace)
      (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (31): ReLU(inplace)
      (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (33): ReLU(inplace)
      (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (35): ReLU(inplace)
      (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=25088, out_features=4096, bias=True)
    (1): ReLU(inplace)
    (2): Dropout(p=0.5)
    (3): Linear(in_features=4096, out_features=4096, bias=True)
    (4): ReLU(inplace)
    (5): Dropout(p=0.5)
    (6): Linear(in_features=4096, out_features=1000, bias=True)
  )
)
==== OPTIMIZER:
SGD (
Parameter Group 0
    dampening: 0
    lr: 0.001
    momentum: 0.9
    nesterov: False
    weight_decay: 0
)

Epoch[0] Iteration[0/2951] Loss: 6.87 Time: 00:00:00:02
Epoch[0] Iteration[10/2951] Loss: 4.01 Time: 00:00:00:06
Epoch[0] Iteration[20/2951] Loss: 3.49 Time: 00:00:00:09
Epoch[0] Iteration[30/2951] Loss: 1.98 Time: 00:00:00:13
Epoch[0] Iteration[40/2951] Loss: 1.69 Time: 00:00:00:17
Epoch[0] Iteration[50/2951] Loss: 1.58 Time: 00:00:00:20
Epoch[0] Iteration[60/2951] Loss: 1.33 Time: 00:00:00:24
Epoch[0] Iteration[70/2951] Loss: 1.41 Time: 00:00:00:27
Epoch[0] Iteration[80/2951] Loss: 1.48 Time: 00:00:00:31
Epoch[0] Iteration[90/2951] Loss: 1.53 Time: 00:00:00:35
Epoch[0] Iteration[100/2951] Loss: 1.50 Time: 00:00:00:38
Epoch[0] Iteration[110/2951] Loss: 1.47 Time: 00:00:00:42
Epoch[0] Iteration[120/2951] Loss: 1.37 Time: 00:00:00:46
Epoch[0] Iteration[130/2951] Loss: 1.74 Time: 00:00:00:49
Epoch[0] Iteration[140/2951] Loss: 1.37 Time: 00:00:00:53
Epoch[0] Iteration[150/2951] Loss: 1.33 Time: 00:00:00:57
Epoch[0] Iteration[160/2951] Loss: 1.60 Time: 00:00:01:00
Epoch[0] Iteration[170/2951] Loss: 1.54 Time: 00:00:01:04
Epoch[0] Iteration[180/2951] Loss: 1.51 Time: 00:00:01:07
Epoch[0] Iteration[190/2951] Loss: 1.64 Time: 00:00:01:11
Epoch[0] Iteration[200/2951] Loss: 1.57 Time: 00:00:01:15
Epoch[0] Iteration[210/2951] Loss: 1.59 Time: 00:00:01:18
Epoch[0] Iteration[220/2951] Loss: 1.22 Time: 00:00:01:22
Epoch[0] Iteration[230/2951] Loss: 1.47 Time: 00:00:01:26
Epoch[0] Iteration[240/2951] Loss: 1.49 Time: 00:00:01:29
Epoch[0] Iteration[250/2951] Loss: 1.33 Time: 00:00:01:33
Epoch[0] Iteration[260/2951] Loss: 1.49 Time: 00:00:01:37
Epoch[0] Iteration[270/2951] Loss: 1.30 Time: 00:00:01:40
Epoch[0] Iteration[280/2951] Loss: 1.67 Time: 00:00:01:44
Epoch[0] Iteration[290/2951] Loss: 1.60 Time: 00:00:01:47
Epoch[0] Iteration[300/2951] Loss: 1.41 Time: 00:00:01:51
Epoch[0] Iteration[310/2951] Loss: 1.45 Time: 00:00:01:55
Epoch[0] Iteration[320/2951] Loss: 1.59 Time: 00:00:01:58
Epoch[0] Iteration[330/2951] Loss: 1.28 Time: 00:00:02:02
Epoch[0] Iteration[340/2951] Loss: 1.54 Time: 00:00:02:06
Epoch[0] Iteration[350/2951] Loss: 1.64 Time: 00:00:02:09
Epoch[0] Iteration[360/2951] Loss: 1.32 Time: 00:00:02:13
Epoch[0] Iteration[370/2951] Loss: 1.06 Time: 00:00:02:17
Epoch[0] Iteration[380/2951] Loss: 1.43 Time: 00:00:02:20
Epoch[0] Iteration[390/2951] Loss: 1.43 Time: 00:00:02:24
Epoch[0] Iteration[400/2951] Loss: 1.12 Time: 00:00:02:28
Epoch[0] Iteration[410/2951] Loss: 1.13 Time: 00:00:02:31
Epoch[0] Iteration[420/2951] Loss: 1.21 Time: 00:00:02:35
Epoch[0] Iteration[430/2951] Loss: 1.85 Time: 00:00:02:39
Epoch[0] Iteration[440/2951] Loss: 1.30 Time: 00:00:02:42
Epoch[0] Iteration[450/2951] Loss: 1.32 Time: 00:00:02:46
Epoch[0] Iteration[460/2951] Loss: 1.25 Time: 00:00:02:50
Epoch[0] Iteration[470/2951] Loss: 1.23 Time: 00:00:02:53
Epoch[0] Iteration[480/2951] Loss: 1.23 Time: 00:00:02:57
Epoch[0] Iteration[490/2951] Loss: 1.23 Time: 00:00:03:01
Epoch[0] Iteration[500/2951] Loss: 1.12 Time: 00:00:03:04
Epoch[0] Iteration[510/2951] Loss: 1.13 Time: 00:00:03:08
Epoch[0] Iteration[520/2951] Loss: 1.44 Time: 00:00:03:12
Epoch[0] Iteration[530/2951] Loss: 1.23 Time: 00:00:03:15
Epoch[0] Iteration[540/2951] Loss: 1.02 Time: 00:00:03:19
Epoch[0] Iteration[550/2951] Loss: 1.13 Time: 00:00:03:23
Epoch[0] Iteration[560/2951] Loss: 1.18 Time: 00:00:03:26
Epoch[0] Iteration[570/2951] Loss: 1.30 Time: 00:00:03:30
Epoch[0] Iteration[580/2951] Loss: 0.99 Time: 00:00:03:34
Epoch[0] Iteration[590/2951] Loss: 1.08 Time: 00:00:03:37
Epoch[0] Iteration[600/2951] Loss: 1.12 Time: 00:00:03:41
Epoch[0] Iteration[610/2951] Loss: 1.09 Time: 00:00:03:45
Epoch[0] Iteration[620/2951] Loss: 1.45 Time: 00:00:03:48
Epoch[0] Iteration[630/2951] Loss: 1.25 Time: 00:00:03:52
Epoch[0] Iteration[640/2951] Loss: 1.01 Time: 00:00:03:56
Epoch[0] Iteration[650/2951] Loss: 0.97 Time: 00:00:03:59
Epoch[0] Iteration[660/2951] Loss: 0.94 Time: 00:00:04:03
Epoch[0] Iteration[670/2951] Loss: 1.03 Time: 00:00:04:07
Epoch[0] Iteration[680/2951] Loss: 0.88 Time: 00:00:04:10
Epoch[0] Iteration[690/2951] Loss: 1.39 Time: 00:00:04:14
Epoch[0] Iteration[700/2951] Loss: 0.97 Time: 00:00:04:18
Epoch[0] Iteration[710/2951] Loss: 1.00 Time: 00:00:04:21
Epoch[0] Iteration[720/2951] Loss: 1.02 Time: 00:00:04:25
Epoch[0] Iteration[730/2951] Loss: 0.96 Time: 00:00:04:29
Epoch[0] Iteration[740/2951] Loss: 1.31 Time: 00:00:04:32
Epoch[0] Iteration[750/2951] Loss: 1.17 Time: 00:00:04:36
Epoch[0] Iteration[760/2951] Loss: 1.47 Time: 00:00:04:40
Epoch[0] Iteration[770/2951] Loss: 0.91 Time: 00:00:04:43
Epoch[0] Iteration[780/2951] Loss: 1.30 Time: 00:00:04:47
Epoch[0] Iteration[790/2951] Loss: 1.17 Time: 00:00:04:51
Epoch[0] Iteration[800/2951] Loss: 0.71 Time: 00:00:04:54
Epoch[0] Iteration[810/2951] Loss: 0.87 Time: 00:00:04:58
Epoch[0] Iteration[820/2951] Loss: 1.18 Time: 00:00:05:02
Epoch[0] Iteration[830/2951] Loss: 0.77 Time: 00:00:05:05
Epoch[0] Iteration[840/2951] Loss: 1.12 Time: 00:00:05:09
Epoch[0] Iteration[850/2951] Loss: 1.11 Time: 00:00:05:13
Epoch[0] Iteration[860/2951] Loss: 0.64 Time: 00:00:05:16
Epoch[0] Iteration[870/2951] Loss: 0.97 Time: 00:00:05:20
Epoch[0] Iteration[880/2951] Loss: 1.07 Time: 00:00:05:24
Epoch[0] Iteration[890/2951] Loss: 1.13 Time: 00:00:05:27
